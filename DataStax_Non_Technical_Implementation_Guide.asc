[width="80%",cols="3,^2,^2,10",options="header"]
|=========================================================
|Date |Duration |Avg HR |Notes

|22-Aug-08 |10:24 | 157 |
Worked out MSHR (max sustainable heart rate) by going hard
for this interval.

|22-Aug-08 |23:03 | 152 |
Back-to-back with previous interval.

|24-Aug-08 |40:00 | 145 |
Moderately hard interspersed with 3x 3min intervals (2min
hard + 1min really hard taking the HR up to 160).

|=========================================================

[width="99%",cols="^2,6,^2,^2,10,10",options="header"]
|=======================================================================
|*Risk Item* |*Description* |*Impact Severity* |*Mitigation Effort* |*Potential Impact* |*Mitigation Technique*
|Test |Test |Test |Test |Test |Test
|=======================================================================


Non-Technical Implementation Guide
==================================

Purpose of the Document
-----------------------

The purpose of this document is to provide an implementation approach to
help guide non-technical team members in leading and supporting
implementing software solutions built on DataStax Enterprise.  The
intended audience of this document is:  Organizational Leaders,
Program/Project Managers, Architects, Team Leaders (Management), and
anyone other team member who is more focused on leading/managing people
and projects vs. technology.   That is, this document is geared to the
people who must answer the question ?who does what? as this document
provides an experience based guide for the who and the what of
implementing software solutions leveraging DataStax Enterprise.  Even
though this is not a technical document, engineering/technology focused
individuals may find it beneficial for project execution context.

The document is broken into the following sections:

.Ordered
. Introduction
. High-level Project Approach
. Risk Management
. Suggested Skills
. Execution Preparation Matrix
. Conclusion

Introduction
------------

The reason this guide is being provided is to enable the transformation
trend that large, established enterprises are experiencing, moving from
traditional methods of business to Internet enterprise methods of
business.  This means that enterprises are moving from methods of
business transactions that are dictated by well defined, strictly
controlled customer interactions in which the business can control how a
customer buys, communicates, receives services, etc. from the business
to an environment where the customer demands an immediate response from
the business to any type of customer requested action, again purchase,
service, etc.  Like any business transformation initiative, the method
to which the transformation is executed is almost equally vital to the
success of the transformation as compared to the technologies that
enable the transformation.  The DataStax Enterprise platform is the
proven technology that is enabling this transformation.  This guide
attempts to provide a proven approach to implementing the
transformation-enabling technology for large, established enterprises.

The goal of this document to provide a non-technical, reference
implementation approach for the implementation of DataStax Enterprise.
The sections of this document have been deliberately selected to help
enterprise leaders understand key project success items, as well as, and
almost as important as, the key risk items that must be carefully
managed to avoid undesired setbacks during implementation.

High-Level Approach
-------------------

The high-level approach for implementing a system on the DataStax
Enterprise platform requires similar approaches as developing a
distributed, customer facing, revenue generating (i.e. mission critical)
application.  For example, planning, communication, and execution are
all required items for the success of mission critical applications and
are also required for implementing systems on DataStax Enterprise.

The following graphic highlights the unique/required steps in a
high-level approach for implementing a solution on DataStax Enterprise.

image:/ApproachOverview.png[
"Approach Overview",width=1000,
link="/ApproachOverview.png"]

_Note:  There are a couple of key project lifecycle phases explicitly
omitted from this diagram, as they contain no DataStax specific items,
Discovery, Planning, and Production Deployment.  Items not depicted in
this graph, such as application functional/security requirements are
assumed to be included in the development approach._

This diagram depicts a methodology agnostic approach to project
implementation.  These phases could be included as major milestones
within a Waterfall, Agile, Kanban, or other project management
methodology. 

The following list provides detail and context for the high-level
approach diagram.

.Bulleted
* Requirements Phase
** DataStax Milestones:
*** Data Model Requirements
*** Security and Encryption Requirements
*** Service Level Agreements
*** Operational Requirements (Monitor and Manage)
*** Search Requirements (Optional – Only for DataStax Search)
*** Analytics Requirements (Optional – Only for DataStax Analytics)
** The pervasive sentiment in the Apache Cassandra community as well as
in the DataStax Enterprise community is that one of the keys to success
with an Apache Cassandra driven platform, like DataStax Enterprise, is
"getting the data model right".  To enable a scalable data model,
specific data model requirements are required.
*** For next generation, transformation, upgrade, etc. projects, a great
starting point for data model requirements is to enable query level
logging from within the existing database.  Then, sort the query logs in
order of occurrence, starting with the most accessed queries first. 
These queries will provide most, if not all, of the requirements needed
to produce the data model for DataStax.
*** For new application/functionality requirements, treat the
requirements phase of the project as you would any API requirements
effort.  That is, define specific Create, Read, Update, and Delete
(CRUD) requirements with a special focus on the Read requirements. 
Specific requirements for the WHERE or BY clause of read operations are
required for successful data model design.
** DataStax Security and Encryption requirements encapsulate the
following areas:
*** Authentication Requirements (i.e. Kerberos, Password, SSL, LDAP,
etc.)
*** Authorization Requirements (i.e. access to Schema, Table, or other
database components)
*** As DataStax Enterprise is a distributed system, encryption
requirements should be defined at 2 distinct levels (note, compression
design choices will occur at this level as well)
**** Client Application to DataStax (the Cluster)
**** Node-to-Node (Inter-Cluster)
** Defining Service Level Agreements (SLAs) for each CRUD operation (in
terms of latency measured in milliseconds), as well as for system uptime
is highly recommended to guide the design and delivery of the solution. 
An absence of SLAs is a project management failure, which has a high
probability of increased project duration and decreased product quality.
** Chances are that you are working to build a mission critical
application that will function at a very large scale serving millions or
more of customer requests per day.  Defining the requirements for the
operational monitoring and management of the system is highly
recommended during this phase of the project.  There is a large risk
that system issues, post production, go either undetected or require an
increased amount of time/duration/effort to resolve if clear operational
requirements are absent from the onset of system implementation.
** If the project's scope includes DataStax Search components, then,
similarly to data model requirements, search requirements are required
at this stage to provide enough clarity to develop the DataStax Search
views (SOLR cores) that will enable search functionality.  The
requirements should be clear enough to determine the fields that will be
searched on and returned in search results.  The requirements should be
clear enough to understand the foundation of how search will be
conducted, i.e. multiple search fields or single search field, the use
of faceted results vs. ranked list results, etc.
** If the project's scope included DataStax Analytics components, then
Analytics requirements should be captured at this time.  Analytics
requirements should incorporate the statistical algorithms, required
data sources, data movement/modifications, security/access, and other
analytical requirements at a clear enough level to enable a thorough
design.
* Design Phase
** DataStax Milestones:
*** Data Model Design
*** Data Access Object Design
*** Data Movement Design
*** Operational Design (Management and Monitoring)
*** Search Design (Optional)
*** Analytics Design (Optional)
** The Data Model design should include the following components in a
format that is clear for all team members to understand.  The following
link will provide in depth reference material for data modeling in
DataStax:
http://www.datastax.com/resources/data-modeling[http://www.datastax.com/resources/data-modeling].
*** Keyspace Design (Replication Strategy, Name)
*** Table Design (Table Names, Partition Keys, Clustering Columns (if
applicable), and physical table properties as necessary (i.e.
encryption, bloom filter settings, etc.)
*** Any relationships between tables.  Note that database joining within
DataStax Enterprise is not technically feasible.  However, relationships
between tables are still import, especially for the application
developers.
** Applications built on DataStax Enterprise are more successful when
applications leverage simple Data Access Objects to encapsulate and
abstract data manipulation logic.  This is opposed to the current trend
in application development, where projects leverage frameworks to
encapsulate, abstract, and represent database components as application
objects, i.e. Hibernate, LinQ, JPA, ORM, etc.  Designing the Data Access
Object, as much as possible, up front will help the application
development team as they build out higher-level functionality.
** Data Movement design includes items such as batch and real-time data
integration between systems, ETL, Change Data Capture, data pipelines,
etc.  Capturing data transformation logic clearly is essential to the
success of data integration initiatives.  Items such as data types,
transformation logic, error handling, look-ups, and data normalization
should be clearly documented as part of Data Movement design.
** Operational Design includes topics such as the tooling and
techniques used to deploy new nodes, configure and upgrade nodes in the
cluster, backup and restore operations, cluster monitoring, Opscenter
use, repairs, alerting, disaster management processes, etc.  Several
organizations leverage a ?playbook? approach to Operational Design.
** It is recommended to incorporate items such as searchable terms,
returned terms, tokenizers, filters, multi-document search terms, etc.
in the Search Design for each searchable view, SOLR Core, that will be
included in the application.  Please see here for more information on
the items available for design with DataStax Search -
http://www.datastax.com/documentation/datastax_enterprise/4.5/datastax_enterprise/srch/srchTOC.html[http://www.datastax.com/documentation/datastax_enterprise/4.5/datastax_enterprise/srch/srchTOC.html].
** When working with DataStax Analytics, it is important to first
determine which Analytics components will be leveraged in the solution. 
Once that decision has been made, then specific, functionally aligned
design items should be produced, such as Hive table structures, Map
Reduce workflows, etc.
* Implementation Phase
** DataStax Milestones:
*** Infrastructure
*** Deployment and Configuration Management Mechanism
*** Software Components (Data Model and Application)
*** Unit Testing of Components
** This phase of the approach is typical for any type of software
project.  This is where "things" are actually built and implemented. 
Building out infrastructure and software components do not require any
special DataStax centric highlights.
** Deployment and Configuration Management Mechanisms are going to be
key to managing a distributed system.  It is recommended that all
operational items are automated, as much as feasible, to optimize the
process of deploying and/or configuring nodes in the cluster.  Tools
like Opscenter, Docker, Vagrant, Chef, Puppet, etc. can be leveraged to
help quickly deliver the operational components necessary to manage the
full software solution.
** Unit Testing of functionality becomes a bit more complex with
distributed systems compared to single node systems.  Specific defects,
such as race conditions, are only observed "at scale".  Because of this,
it is recommended that unit testing be executed over a small cluster,
that contain more than a single node.  Tools such as
https://github.com/pcmanus/ccm[ccm] can be used by developers to
automate the process of quickly launching test clusters as part of a
unit test.
* Pre-Production Testing Phase
** DataStax Milestones:
*** Defect tracking items (JIRA, Log of Issues, etc.)
*** Operational readiness checklist completed
** This is perhaps the most critical phase of this approach.  This
phase of the project will allow the project team to identify actual
production issues prior to going to production.  As stated in the unit
testing section of this document, specific defects will not be observed
until the software solution is functioning "at scale" under normal and
extraordinary conditions for a period of time.  This phase of the
project is deliberately included in the approach to enable
identification of "at scale" issues so they can be resolved prior to the
Production release of the application.
** This phase of the project should be conducted over a recommended 2
week period, where the application runs at production scale for at least
1 week.  Only observations should be made during this period of the
project.  It may take several iterations of configuration, code change,
refactoring to enable the application to execute for a full week.  The
1 week recommendation is being provided to ensure there are enough data
points to conclude that the application, including infrastructure, is
adequate to handle a production workload.  Apache Cassandra needs to be
stressed for this amount of time to determine if read performance
degrades, due to Compaction design items, or remains acceptable.
** Here is a list of items that should be included in an Operational
Readiness Checklist for DataStax Enterprise:
*** Replace a downed node and a dead seed node
*** Configure and execute repair (ensure repair completes within
http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/tabProp.html[GC_Grace_Period])
*** Add a node to a cluster
*** Replace a downed Data Center
*** Add a Data Center to the cluster
*** Decommission a node
*** Restore a backup
*** At a Cluster Level and Per Node Level, report on errors, throughput,
latency, resource saturation, bottlenecks, compactions, flushes, and
health
* Scale and Enhancements Phase
** This phase is included in the approach to highlight the normal
operational mode of an application built on DataStax Enterprise.  When
the time comes, this will be very predictable, add nodes to add capacity
to the system.  Scaling with DataStax Enterprise is as simple as that.

As mentioned above, this approach is methodology agnostic.  The stages
in the approach can be executed as single, individual phases in a
Waterfall approach or by iterating over each phase in small,
?horizontal? slices of functionality that include a small part of each
phase.  Please note that Pre-Production testing should be executed as a
single phase that includes all functionality that will be included in a
Production deployment.

There is also an approach that has been presented that shows how small,
agile teams can go from Prototype (PoC) to Production without much
refactoring.  Here is a link to the approach -
http://www.slideshare.net/planetcassandra/jake-luiciani-poc-to-production-c[http://www.slideshare.net/planetcassandra/jake-luiciani-poc-to-production-c]

The attached presentation is intended for technical audiences.  It
provides some good details on data modeling as well as Pre-Production
testing.  The point of this presentation is that, if the PoC is well
constructed, then you can move directly into the Pre-Production testing
phase of this approach, skipping the requirements through implementation
phases.  This approach highlights the scaling advantage of  Apace
Cassandra and DataStax Enterprise.

Risk Management
---------------

What would a technology project be without risk?  That?s a trick
question.  Because, without risk, there are no rewards with technology
projects.  This is especially true for the types of transformational
applications that are being built on top of the DataStax Enterprise
platform.  Because of the scale that DataStax Enterprise can enable,
millions of transactions per second, 10s of Petabytes of ?live? data, a
small risk can quickly turn into a large issue if the risk is not
identified and managed.

This section of the document highlights some key areas for project risk
management for DataStax Enterprise and Apache Cassandra.  Like the
approach section of this document, the Risk Management section is not an
exhaustive list of risk management items for large, distributed
applications, only DataStax centric items are covered.
